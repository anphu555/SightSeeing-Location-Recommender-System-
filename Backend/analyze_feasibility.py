"""
Ph√¢n t√≠ch kh·∫£ nƒÉng ƒë·∫°t Precision@10 = 40%
"""
from sqlmodel import Session, select
from app.database import engine
from app.schemas import User, Rating, Place
import numpy as np

with Session(engine) as session:
    # Th·ªëng k√™ data quality
    users = session.exec(select(User)).all()
    ratings = session.exec(select(Rating)).all()
    places = session.exec(select(Place)).all()
    
    print("=== PH√ÇN T√çCH DATA QUALITY ===\n")
    print(f"T·ªïng users: {len(users)}")
    print(f"T·ªïng ratings: {len(ratings)}")
    print(f"T·ªïng places: {len(places)}")
    print(f"Sparsity: {len(ratings) / (len(users) * len(places)) * 100:.4f}%")
    
    # Rating distribution
    scores = [r.score for r in ratings]
    print(f"\nüìä Rating Distribution:")
    print(f"  Mean: {np.mean(scores):.2f}")
    print(f"  Median: {np.median(scores):.2f}")
    for score in [1.0, 2.0, 3.0, 4.0, 5.0]:
        count = sum(1 for s in scores if s == score)
        print(f"  Score {score}: {count} ({count/len(scores)*100:.1f}%)")
    
    # Ratings per user
    user_ratings = {}
    for rating in ratings:
        user_ratings[rating.user_id] = user_ratings.get(rating.user_id, 0) + 1
    
    rating_counts = list(user_ratings.values())
    print(f"\nüìä Ratings per User:")
    print(f"  Mean: {np.mean(rating_counts):.1f}")
    print(f"  Median: {np.median(rating_counts):.1f}")
    print(f"  Min: {min(rating_counts)}")
    print(f"  Max: {max(rating_counts)}")
    print(f"  Users with 20+ ratings: {sum(1 for c in rating_counts if c >= 20)}")
    
    # Ratings per place
    place_ratings = {}
    for rating in ratings:
        place_ratings[rating.place_id] = place_ratings.get(rating.place_id, 0) + 1
    
    place_counts = list(place_ratings.values())
    print(f"\nüìä Ratings per Place:")
    print(f"  Mean: {np.mean(place_counts):.1f}")
    print(f"  Median: {np.median(place_counts):.1f}")
    print(f"  Min: {min(place_counts)}")
    print(f"  Max: {max(place_counts)}")
    print(f"  Places with 0 ratings: {len(places) - len(place_ratings)}")
    print(f"  Places with 10+ ratings: {sum(1 for c in place_counts if c >= 10)}")
    
    # Tag analysis
    all_tags = []
    for place in places:
        if place.tags:
            all_tags.extend(place.tags)
    
    unique_tags = set(all_tags)
    print(f"\nüìä Tag Statistics:")
    print(f"  Unique tags: {len(unique_tags)}")
    print(f"  Total tag assignments: {len(all_tags)}")
    print(f"  Avg tags per place: {len(all_tags) / len(places):.1f}")
    
    # Tag co-occurrence (user preferences)
    print(f"\nüìä User Preference Patterns:")
    user_tag_counts = {}
    for rating in ratings:
        if rating.score >= 4.0:
            place = session.get(Place, rating.place_id)
            if place and place.tags:
                if rating.user_id not in user_tag_counts:
                    user_tag_counts[rating.user_id] = {}
                for tag in place.tags:
                    user_tag_counts[rating.user_id][tag] = user_tag_counts[rating.user_id].get(tag, 0) + 1
    
    # Users with clear preferences (one tag appears 3+ times)
    users_with_clear_prefs = 0
    for user_id, tag_counts in user_tag_counts.items():
        if any(count >= 3 for count in tag_counts.values()):
            users_with_clear_prefs += 1
    
    print(f"  Users v·ªõi clear preferences: {users_with_clear_prefs}/{len(users)} ({users_with_clear_prefs/len(users)*100:.1f}%)")
    
    print("\n=== ƒê√ÅNH GI√Å KH·∫¢ NƒÇNG ƒê·∫†T 40% ===\n")
    
    # Factors
    print("‚úÖ Y·∫æU T·ªê THU·∫¨N L·ª¢I:")
    print(f"  - Data sparsity th·∫•p ({len(ratings) / (len(users) * len(places)) * 100:.4f}%)")
    print(f"  - C√≥ {len(unique_tags)} tags ƒë·ªÉ ph√¢n lo·∫°i")
    print(f"  - {users_with_clear_prefs/len(users)*100:.1f}% users c√≥ clear preferences")
    
    print("\n‚ö†Ô∏è Y·∫æU T·ªê KH√ì KHƒÇN:")
    if np.mean(rating_counts) < 15:
        print(f"  - Trung b√¨nh ch·ªâ {np.mean(rating_counts):.1f} ratings/user (√≠t data)")
    if len(places) - len(place_ratings) > 100:
        print(f"  - {len(places) - len(place_ratings)} places ch∆∞a c√≥ rating (cold-start)")
    print(f"  - Sparsity cao: ch·ªâ {len(ratings) / (len(users) * len(places)) * 100:.4f}% cells c√≥ data")
    
    print("\nüí° CHI·∫æN L∆Ø·ª¢C ƒê·ªÄ XU·∫§T:")
    print("  1. ‚úÖ C·∫¢I THI·ªÜN TF-IDF: n-grams, tag weighting")
    print("  2. ‚úÖ COLLABORATIVE FILTERING: Item-based CF m·∫°nh h∆°n")
    print("  3. ‚úÖ POPULARITY: Time-decay popularity")
    print("  4. ‚≠ê MATRIX FACTORIZATION: SVD/ALS cho implicit feedback")
    print("  5. ‚≠ê DEEP LEARNING: Train Two-Tower model ƒë√∫ng c√°ch")
    print("  6. ‚≠ê ENSEMBLE: K·∫øt h·ª£p nhi·ªÅu models")
    print("  7. ‚úÖ RE-RANKING: Diversity + freshness + business rules")
    
    print("\nüéØ K·∫æT LU·∫¨N:")
    print(f"  V·ªõi data hi·ªán t·∫°i ({len(ratings)} ratings, {len(users)} users):")
    print(f"  - Precision@10 = 25-30%: ‚úÖ FEASIBLE v·ªõi c·∫£i ti·∫øn current approach")
    print(f"  - Precision@10 = 35-40%: ‚≠ê POSSIBLE v·ªõi Matrix Factorization ho·∫∑c DL")
    print(f"  - Precision@10 = 40%+: ‚ö†Ô∏è CHALLENGING - c·∫ßn th√™m data ho·∫∑c context features")
