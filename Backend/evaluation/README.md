# ğŸ“Š EVALUATION - ThÆ° má»¥c ÄÃ¡nh giÃ¡ Há»‡ thá»‘ng

ThÆ° má»¥c nÃ y chá»©a táº¥t cáº£ cÃ¡c cÃ´ng cá»¥ vÃ  tÃ i liá»‡u liÃªn quan Ä‘áº¿n viá»‡c Ä‘Ã¡nh giÃ¡ (evaluation) há»‡ thá»‘ng recommendation.

## ğŸ“ Cáº¥u trÃºc

### ğŸ”§ Scripts chÃ­nh

| File | MÃ´ táº£ |
|------|-------|
| **evaluate_recsys.py** | Script chÃ­nh Ä‘á»ƒ cháº¡y evaluation, tÃ­nh toÃ¡n metrics (Precision, Recall, NDCG, MAP) |
| **analyze_evaluation_methodology.py** | PhÃ¢n tÃ­ch vÃ  giáº£i thÃ­ch phÆ°Æ¡ng phÃ¡p evaluation (Train/Test split, Ground truth) |
| **analyze_rating_categories.py** | PhÃ¢n tÃ­ch category consistency - kiá»ƒm tra user cÃ³ rate Ä‘Ãºng thá»ƒ loáº¡i khÃ´ng |

### ğŸ“„ TÃ i liá»‡u

| File | MÃ´ táº£ |
|------|-------|
| **EVALUATION_EXPLAINED.md** | â­ Giáº£i thÃ­ch chi tiáº¿t phÆ°Æ¡ng phÃ¡p evaluation (báº¯t Ä‘áº§u tá»« Ä‘Ã¢y!) |
| **EVALUATION_GUIDE.md** | HÆ°á»›ng dáº«n chi tiáº¿t cÃ¡ch cháº¡y evaluation |
| **EVALUATION_REPORT.md** | BÃ¡o cÃ¡o káº¿t quáº£ evaluation |
| **SETUP_EVALUATION.md** | HÆ°á»›ng dáº«n setup mÃ´i trÆ°á»ng evaluation |

### ğŸ“Š Káº¿t quáº£

| File | MÃ´ táº£ |
|------|-------|
| **evaluation_detailed.csv** | Káº¿t quáº£ chi tiáº¿t cho tá»«ng user |
| **evaluation_results.json** | Tá»•ng há»£p káº¿t quáº£ metrics |
| **evaluation_output.txt** | Output logs cá»§a quÃ¡ trÃ¬nh evaluation |

### ğŸš€ Scripts cháº¡y

| File | MÃ´ táº£ |
|------|-------|
| **run_evaluation.bat** | Cháº¡y evaluation trÃªn Windows (CMD) |
| **run_evaluation.sh** | Cháº¡y evaluation trÃªn Linux/Mac |
| **run_full_evaluation.sh** | Cháº¡y full evaluation vá»›i táº¥t cáº£ tests |
| **Run-Evaluation.ps1** | Cháº¡y evaluation trÃªn Windows (PowerShell) |

## ğŸš€ CÃ¡ch sá»­ dá»¥ng nhanh

### 1. Hiá»ƒu phÆ°Æ¡ng phÃ¡p evaluation

```bash
# Äá»c document giáº£i thÃ­ch
cat EVALUATION_EXPLAINED.md

# Hoáº·c cháº¡y phÃ¢n tÃ­ch
python analyze_evaluation_methodology.py
```

### 2. PhÃ¢n tÃ­ch data hiá»‡n táº¡i

```bash
# Kiá»ƒm tra category consistency
python analyze_rating_categories.py
```

### 3. Cháº¡y evaluation

```bash
# Windows PowerShell
.\Run-Evaluation.ps1

# Windows CMD
.\run_evaluation.bat

# Linux/Mac
./run_evaluation.sh
```

### 4. Xem káº¿t quáº£

```bash
# Xem káº¿t quáº£ tá»•ng quan
cat evaluation_output.txt

# Xem chi tiáº¿t tá»«ng user
cat evaluation_detailed.csv
```

## ğŸ“‹ Quy trÃ¬nh Evaluation

```
1. Chuáº©n bá»‹ dá»¯ liá»‡u
   â””â”€ Táº¡o test data vá»›i ../create_test_data.py

2. Chia Train/Test (80/20)
   â””â”€ Train: Model há»c tá»« Ä‘Ã¢y
   â””â”€ Test: Giáº¥u Ä‘i, dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ (Ground Truth)

3. Model dá»± Ä‘oÃ¡n
   â””â”€ Recommend top-K items cho má»—i user

4. So sÃ¡nh vá»›i Ground Truth
   â””â”€ TÃ­nh Precision@K, Recall@K, NDCG@K, MAP

5. Xuáº¥t káº¿t quáº£
   â””â”€ evaluation_detailed.csv
   â””â”€ evaluation_results.json
   â””â”€ evaluation_output.txt
```

## ğŸ“Š Metrics Ä‘Æ°á»£c Ä‘o

| Metric | Ã nghÄ©a | GiÃ¡ trá»‹ tá»‘t |
|--------|---------|-------------|
| **Precision@K** | % recommendations Ä‘Ãºng trong top-K | Cao hÆ¡n tá»‘t (0-1) |
| **Recall@K** | % relevant items Ä‘Æ°á»£c tÃ¬m tháº¥y | Cao hÆ¡n tá»‘t (0-1) |
| **F1@K** | Harmonic mean cá»§a Precision & Recall | Cao hÆ¡n tá»‘t (0-1) |
| **NDCG@K** | ÄÃ¡nh giÃ¡ ranking quality | Cao hÆ¡n tá»‘t (0-1) |
| **MAP** | Mean Average Precision | Cao hÆ¡n tá»‘t (0-1) |
| **Coverage** | % items Ä‘Æ°á»£c recommend | 0.3-0.5 lÃ  tá»‘t |
| **Diversity** | Äá»™ Ä‘a dáº¡ng recommendations | 0.5-0.8 lÃ  tá»‘t |

## ğŸ¯ Káº¿t quáº£ hiá»‡n táº¡i

Xem chi tiáº¿t trong [EVALUATION_REPORT.md](./EVALUATION_REPORT.md)

**TÃ³m táº¯t:**
- Precision@5: ~0.26
- Recall@5: ~0.27
- NDCG@5: ~0.32
- MAP: ~0.19

â†’ Algorithm hoáº¡t Ä‘á»™ng tá»‘t, cÃ³ thá»ƒ cáº£i thiá»‡n thÃªm báº±ng cÃ¡ch tÄƒng dá»¯ liá»‡u training

## ğŸ’¡ FAQs

### â“ Ground truth lÃ  gÃ¬?
â†’ Xem [EVALUATION_EXPLAINED.md](./EVALUATION_EXPLAINED.md) section "GIáº¢I ÄÃP NGáº®N Gá»ŒN"

### â“ LÃ m sao biáº¿t precision khi chá»‰ cÃ³ ratings?
â†’ Sá»­ dá»¥ng Train/Test split - giáº¥u má»™t pháº§n ratings lÃ m ground truth

### â“ User rate Ä‘Ãºng thá»ƒ loáº¡i khÃ´ng?
â†’ Cháº¡y `python analyze_rating_categories.py` Ä‘á»ƒ kiá»ƒm tra

### â“ Cáº£i thiá»‡n káº¿t quáº£ evaluation nhÆ° tháº¿ nÃ o?
â†’ Táº¡o dá»¯ liá»‡u test tá»‘t hÆ¡n vá»›i `../create_improved_test_data.py`

## ğŸ”— LiÃªn káº¿t

- [Quay vá» Backend](../)
- [Táº¡o test data](../create_test_data.py)
- [Recommendation System](../app/routers/recsysmodel.py)

---

**Cáº­p nháº­t:** December 17, 2025
**TÃ¡c giáº£:** Evaluation Team
