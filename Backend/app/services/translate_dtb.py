import json
import csv
import asyncio
from groq import Groq
from fastapi.concurrency import run_in_threadpool
import time
from datetime import datetime



# ====== CSV translate ======
INPUT_CSV = "data896-899.csv"
OUTPUT_CSV = "data896-899_en.csv"
CHECKPOINT_CSV = "data896-899_en.csv"


# ====== CONFIG ======
API_KEY = "GROQ_API_KEY"
# MODEL = "llama-3.3-70b-versatile"
# MODEL = "qwen/qwen3-32b"
MODEL = "openai/gpt-oss-120b"



# Rate limiting - ∆ØU TI√äN TPD (Tokens Per Day)
MAX_TOKENS_PER_DAY = 95000  # Gi·ªõi h·∫°n 95k/100k ƒë·ªÉ an to√†n
ESTIMATED_TOKENS_PER_ROW = 150  # ∆Ø·ªõc t√≠nh token cho m·ªói row
MAX_ROWS_PER_DAY = MAX_TOKENS_PER_DAY // ESTIMATED_TOKENS_PER_ROW  # ~633 rows/day

# Rate limiting th·ª© c·∫•p
MAX_RPM = 25
BATCH_SIZE = 25
DELAY_BETWEEN_BATCHES = 65

client = Groq(api_key=API_KEY)

SYSTEM_PROMPT = """
You are a professional travel writer and translator specializing in Vietnamese tourism content for international audiences.

TRANSLATION PRINCIPLES:
1. Write naturally for native English speakers - avoid literal translations
2. Use engaging, descriptive language that inspires travel
3. Simplify complex Vietnamese cultural terms with brief explanations
4. Break long sentences into shorter, readable ones
5. Keep the tone warm, inviting, and informative
6. Preserve specific names of places, but skip the marks in Vietnamese and translate their meanings in parentheses when helpful

SPECIFIC GUIDELINES:
- Historical dates: Use simple format "built in 1070" instead of "the second year of Thien Vu reign"
- Vietnamese terms: Either translate them or keep the term with a brief English explanation
- Measurements: Keep meters (m), hectares (ha) as-is
- Species names: Use common English names, scientific names in parentheses only if important
- Architecture terms: Use simple English equivalents (e.g., "pagoda" not "ch√πa")
- Preserve "|||" separators exactly as-is

CRITICAL CSV FORMATTING RULES:
- DO NOT add line breaks or indentation in the description
- Each paragraph/section should be separated ONLY by "|||"
- Write continuously without pressing Enter/Return
- Use single spaces between words, no extra whitespace

OUTPUT FORMAT:
- Only translate "title" and "description" fields
- Output MUST be valid JSON only - no markdown, no extra text
- NO line breaks inside description text
- Preserve all "|||" separators
- If a field is empty, leave it empty

GOOD EXAMPLES:

Input: {"title": "Ch√πa M·ªôt C·ªôt", "description": "Ch√πa M·ªôt C·ªôt ƒë∆∞·ª£c x√¢y d·ª±ng nƒÉm 1049 d∆∞·ªõi th·ªùi vua L√Ω Th√°i T√¥ng. Ch√πa c√≥ h√¨nh d·∫°ng ƒë·ªôc ƒë√°o nh∆∞ b√¥ng sen n·ªü tr√™n m·∫∑t n∆∞·ªõc."}
Output: {"title": "One Pillar Pagoda", "description": "Built in 1049 under Emperor Ly Thai Tong, this unique pagoda rises from the water like a lotus blossom in full bloom."}

Input: {"title": "R·ª´ng tr√†m Tr√† S∆∞", "description": "R·ª´ng tr√†m Tr√† S∆∞ c√≥ di·ªán t√≠ch g·∫ßn 850ha, l√† n∆°i sinh s·ªëng c·ªßa 70 lo√†i chim thu·ªôc 13 b·ªô v√† 31 h·ªç.|||M√πa n∆∞·ªõc n·ªïi l√† th·ªùi ƒëi·ªÉm ƒë·∫πp nh·∫•t ƒë·ªÉ thƒÉm r·ª´ng."}
Output: {"title": "Tra Su Cajuput Forest", "description": "Spanning nearly 850 hectares, Tra Su Forest is a sanctuary for 70 bird species, creating a paradise for nature lovers and birdwatchers.|||The flooding season is the best time to visit the forest."}

Now translate this input JSON:
"""


def _call_groq(user_text: str):
    if not API_KEY:
        raise Exception("GROQ_API_KEY is missing")
        
    completion = client.chat.completions.create(
        model=MODEL,
        temperature=0.3,
        max_tokens=4000,  # ‚úÖ QUAN TR·ªåNG: TƒÉng limit ƒë·ªÉ d·ªãch h·∫øt description d√†i
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_text},
        ],
    )
    content = completion.choices[0].message.content
    
    # L·∫•y th√¥ng tin token usage
    usage = completion.usage
    return json.loads(content), usage


async def extract_with_groq(user_text: str):
    try:
        data, usage = await run_in_threadpool(_call_groq, user_text)
        return data, usage
    except Exception as e:
        error_msg = str(e)
        if "rate_limit_exceeded" in error_msg:
            print(f"\nüö´ RATE LIMIT HIT!")
            if "tokens per day" in error_msg.lower():
                print(f"   TPD limit reached. Need to wait or upgrade.")
            elif "requests per minute" in error_msg.lower():
                print(f"   RPM limit reached. Slowing down...")
        print(f"‚ùå Error: {error_msg[:200]}")
        return None, None





async def translate_csv(max_rows=None):
    """
    D·ªãch CSV v·ªõi token tracking v√† rate limiting
    
    Args:
        max_rows: S·ªë d√≤ng t·ªëi ƒëa mu·ªën d·ªãch (None = kh√¥ng gi·ªõi h·∫°n)
    """
    
    # ƒê·ªçc t·∫•t c·∫£ rows
    print(f"üìñ Reading {INPUT_CSV}...")
    all_rows = []
    with open(INPUT_CSV, newline='', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        all_rows = list(reader)
    
    total_rows = len(all_rows)
    print(f"‚úÖ Found {total_rows} rows")
    
    # √Åp d·ª•ng gi·ªõi h·∫°n n·∫øu c√≥
    if max_rows:
        total_rows = min(total_rows, max_rows)
        all_rows = all_rows[:total_rows]
        print(f"‚ö†Ô∏è  Limiting to {total_rows} rows (token budget)")
    
    # Ki·ªÉm tra checkpoint
    translated_rows = []
    start_index = 0
    total_tokens_used = 0
    
    try:
        with open(CHECKPOINT_CSV, newline='', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            translated_rows = list(reader)
            start_index = len(translated_rows)
            print(f"üìå Resuming from row {start_index}")
    except FileNotFoundError:
        print("üÜï Starting fresh translation")
    
    # Token tracking
    print(f"\nüí° Token budget: {MAX_TOKENS_PER_DAY:,} tokens/day")
    print(f"üí° Estimated: ~{ESTIMATED_TOKENS_PER_ROW} tokens/row")
    print(f"üí° Safe limit: {MAX_ROWS_PER_DAY} rows/day")
    
    # D·ªãch t·ª´ng batch
    for batch_num in range(start_index // BATCH_SIZE, (total_rows + BATCH_SIZE - 1) // BATCH_SIZE):
        batch_start = batch_num * BATCH_SIZE
        batch_end = min(batch_start + BATCH_SIZE, total_rows)
        batch_rows = all_rows[batch_start:batch_end]
        
        print(f"\n{'='*60}")
        print(f"üîÑ Batch {batch_num + 1} (rows {batch_start + 1}-{batch_end})")
        print(f"   Tokens used so far: {total_tokens_used:,}")
        print(f"{'='*60}")
        
        batch_start_time = time.time()
        batch_tokens = 0
        
        # D·ªãch t·ª´ng row
        for i, row in enumerate(batch_rows):
            current_row = batch_start + i + 1
            
            title = row.get("title", "")
            desc = row.get("description", "")
            
            # ‚úÖ KI·ªÇM TRA description d√†i
            desc_length = len(desc)
            if desc_length > 3000:
                print(f"  ‚ö†Ô∏è  Row {current_row}: Long description ({desc_length} chars), splitting...")
                
                # Chia description theo separator "|||"
                desc_parts = desc.split("|||")
                translated_parts = []
                
                for part_idx, part in enumerate(desc_parts):
                    if not part.strip():
                        translated_parts.append("")
                        continue
                        
                    json_text = json.dumps({
                        "title": "" if part_idx > 0 else title,  # Ch·ªâ d·ªãch title ·ªü part ƒë·∫ßu
                        "description": part.strip()
                    }, ensure_ascii=False)
                    
                    translated, usage = await extract_with_groq(json_text)
                    
                    if translated and usage:
                        batch_tokens += usage.total_tokens
                        total_tokens_used += usage.total_tokens
                        translated_parts.append(translated.get("description", part))
                        
                        if part_idx == 0 and translated.get("title"):
                            title = translated.get("title")
                        
                        print(f"     Part {part_idx + 1}/{len(desc_parts)}: {usage.total_tokens} tokens")
                    else:
                        translated_parts.append(part)
                    
                    # Delay nh·ªè gi·ªØa c√°c parts
                    await asyncio.sleep(0.5)
                
                # Merge l·∫°i
                new_row = row.copy()
                new_row["title"] = title
                new_row["description"] = "|||".join(translated_parts)
                translated_rows.append(new_row)
                
                print(f"  ‚úÖ Row {current_row}/{total_rows}: {title[:40]}... (split into {len(desc_parts)} parts)")
                
            else:
                # Description ng·∫Øn - d·ªãch b√¨nh th∆∞·ªùng
                json_text = json.dumps({
                    "title": title,
                    "description": desc
                }, ensure_ascii=False)
                
                translated, usage = await extract_with_groq(json_text)
                
                if translated and usage:
                    batch_tokens += usage.total_tokens
                    total_tokens_used += usage.total_tokens
                    
                    new_row = row.copy()
                    new_row["title"] = translated.get("title", title)
                    new_row["description"] = translated.get("description", desc)
                    translated_rows.append(new_row)
                    
                    title_preview = new_row['title'][:40] + "..." if len(new_row['title']) > 40 else new_row['title']
                    print(f"  ‚úÖ Row {current_row}/{total_rows}: {title_preview}")
                    print(f"     Tokens: {usage.total_tokens} (total: {total_tokens_used:,})")
                else:
                    translated_rows.append(row)
                    print(f"  ‚ö†Ô∏è  Row {current_row}/{total_rows}: Failed, keeping original")
            
            # KI·ªÇM TRA token limit
            if total_tokens_used >= MAX_TOKENS_PER_DAY:
                print(f"\n‚ö†Ô∏è  TOKEN LIMIT REACHED ({total_tokens_used:,}/{MAX_TOKENS_PER_DAY:,})")
                print(f"   Saving progress and stopping...")
                save_checkpoint(translated_rows)
                save_final_output(translated_rows)
                print(f"\nüìä Translated {len(translated_rows)}/{len(all_rows)} rows")
                print(f"üíæ Resume tomorrow or upgrade plan!")
                return
            
            # L∆∞u checkpoint m·ªói 5 rows
            if current_row % 5 == 0:
                save_checkpoint(translated_rows)
        
        # Batch summary
        batch_elapsed = time.time() - batch_start_time
        print(f"\nüìä Batch summary:")
        print(f"   Tokens used: {batch_tokens}")
        print(f"   Time: {batch_elapsed:.1f}s")
        
        # Delay gi·ªØa c√°c batch
        if batch_end < total_rows:
            wait_time = max(0, DELAY_BETWEEN_BATCHES - batch_elapsed)
            if wait_time > 0:
                print(f"‚è≥ Waiting {wait_time:.1f}s...")
                await asyncio.sleep(wait_time)
    
    # L∆∞u k·∫øt qu·∫£ cu·ªëi
    save_final_output(translated_rows)
    
    # ‚úÖ VALIDATION: Ki·ªÉm tra s·ªë c·ªôt
    print(f"\nüîç Validating output CSV...")
    try:
        with open(OUTPUT_CSV, newline='', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            output_rows = list(reader)
            
        print(f"‚úÖ Validation passed:")
        print(f"   Input rows: {len(all_rows)}")
        print(f"   Output rows: {len(output_rows)}")
        print(f"   Columns: {list(output_rows[0].keys()) if output_rows else 'N/A'}")
        
        # Ki·ªÉm tra c·ªôt
        if output_rows:
            expected_cols = list(all_rows[0].keys())
            actual_cols = list(output_rows[0].keys())
            if expected_cols != actual_cols:
                print(f"‚ö†Ô∏è  WARNING: Column mismatch!")
                print(f"   Expected: {expected_cols}")
                print(f"   Got: {actual_cols}")
    except Exception as e:
        print(f"‚ùå Validation failed: {e}")
    
    print(f"\nüéâ Translation complete!")
    print(f"üìä Final stats:")
    print(f"   Rows: {len(translated_rows)}/{len(all_rows)}")
    print(f"   Tokens: {total_tokens_used:,}")


def save_checkpoint(rows):
    """L∆∞u checkpoint - ƒê·∫¢M B·∫¢O format CSV ƒë√∫ng"""
    if rows:
        fieldnames = rows[0].keys()
        with open(CHECKPOINT_CSV, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)
            writer.writeheader()
            for row in rows:
                # ‚úÖ L√†m s·∫°ch description: x√≥a xu·ªëng d√≤ng kh√¥ng mong mu·ªën
                if 'description' in row:
                    desc = row['description']
                    # Gi·ªØ nguy√™n separator "|||", nh∆∞ng x√≥a \n trong m·ªói ph·∫ßn
                    parts = desc.split('|||')
                    cleaned_parts = []
                    for part in parts:
                        # X√≥a xu·ªëng d√≤ng, gi·ªØ m·ªôt kho·∫£ng tr·∫Øng
                        cleaned = ' '.join(part.split())
                        cleaned_parts.append(cleaned)
                    row['description'] = '|||'.join(cleaned_parts)
                
                writer.writerow(row)


def save_final_output(rows):
    """L∆∞u file cu·ªëi c√πng - ƒê·∫¢M B·∫¢O format CSV ƒë√∫ng"""
    if rows:
        fieldnames = rows[0].keys()
        with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)
            writer.writeheader()
            for row in rows:
                # ‚úÖ L√†m s·∫°ch description
                if 'description' in row:
                    desc = row['description']
                    parts = desc.split('|||')
                    cleaned_parts = []
                    for part in parts:
                        cleaned = ' '.join(part.split())
                        cleaned_parts.append(cleaned)
                    row['description'] = '|||'.join(cleaned_parts)
                
                writer.writerow(row)
        print(f"üíæ Saved to {OUTPUT_CSV}")


# ====== Run ======
if __name__ == "__main__":
    print("=" * 60)
    print("üåè VIETNAM PLACES CSV TRANSLATOR")
    print("=" * 60)
    print(f"‚öôÔ∏è  Model: {MODEL}")
    print(f"‚öôÔ∏è  Token limit: {MAX_TOKENS_PER_DAY:,}/day")
    print(f"‚öôÔ∏è  Safe rows/day: ~{MAX_ROWS_PER_DAY}")
    print("=" * 60)
    
    # T√ôY CH·ªåN: Gi·ªõi h·∫°n s·ªë d√≤ng d·ªãch m·ªói l·∫ßn ch·∫°y
    # Uncomment d√≤ng d∆∞·ªõi n·∫øu mu·ªën d·ªãch t·ªëi ƒëa N d√≤ng/ng√†y
    # MAX_ROWS_TO_TRANSLATE = 600  
    MAX_ROWS_TO_TRANSLATE = None  # None = d·ªãch h·∫øt (ƒë·∫øn khi h·∫øt token)
    
    start_time = datetime.now()
    asyncio.run(translate_csv(max_rows=MAX_ROWS_TO_TRANSLATE))
    end_time = datetime.now()
    
    elapsed = (end_time - start_time).total_seconds()
    print(f"\n‚è±Ô∏è  Total time: {elapsed/60:.1f} minutes")