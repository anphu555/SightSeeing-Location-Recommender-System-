import json
import csv
import asyncio
from groq import Groq
from fastapi.concurrency import run_in_threadpool
import time
from datetime import datetime

# ====== CONFIG ======
API_KEY = "GROQ_API_KEY"
MODEL = "meta-llama/llama-4-scout-17b-16e-instruct"

# Rate limiting - ∆ØU TI√äN TPD (Tokens Per Day)
MAX_TOKENS_PER_DAY = 395000  # Gi·ªõi h·∫°n 95k/100k ƒë·ªÉ an to√†n
ESTIMATED_TOKENS_PER_ROW = 150  # ∆Ø·ªõc t√≠nh token cho m·ªói row
MAX_ROWS_PER_DAY = MAX_TOKENS_PER_DAY // ESTIMATED_TOKENS_PER_ROW  # ~633 rows/day

# Rate limiting th·ª© c·∫•p
MAX_RPM = 25
BATCH_SIZE = 25
DELAY_BETWEEN_BATCHES = 65

client = Groq(api_key=API_KEY)

SYSTEM_PROMPT = """
You are a JSON translator. Your output must be ONLY one valid JSON object.

TASK:
Translate Vietnamese to English for the fields "title" and "description" ONLY. Do not change anything else.

TRANSLATION RULES:
1. Translate naturally, not word-for-word.
2. Keep Vietnamese place names but remove diacritics (VƒÉn Mi·∫øu ‚Üí Van Mieu).
3. Simplify dates (e.g., "built in 1070").
4. Break long Vietnamese sentences into shorter, clear English sentences.
5. Preserve the exact "|||" separators in the description.

STRICT JSON RULES:
- Output MUST start with "{" and end with "}".
- Output MUST be valid JSON.
- NO text before or after the JSON.
- NO markdown, NO code blocks.
- NO line breaks inside any string ‚Äî replace with a space.
- Escape characters correctly: " ‚Üí \\" and \\ ‚Üí \\\\

Example Input:
{"title":"VƒÉn Mi·∫øu","description":"VƒÉn Mi·∫øu ƒë∆∞·ª£c x√¢y nƒÉm 1070 d∆∞·ªõi vua L√Ω Th√°nh T√¥ng.|||ƒê√¢y l√† di t√≠ch quan tr·ªçng."}

Example Output:
{"title":"Temple of Literature","description":"Built in 1070 under King Ly Thanh Tong.|||This is an important historical site."}

Now translate:
"""


def _call_groq(user_text: str):
    if not API_KEY:
        raise Exception("GROQ_API_KEY is missing")
        
    completion = client.chat.completions.create(
        model=MODEL,
        temperature=0.3,
        max_tokens=4000,  # ‚úÖ QUAN TR·ªåNG: TƒÉng limit ƒë·ªÉ d·ªãch h·∫øt description d√†i
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_text},
        ],
    )
    content = completion.choices[0].message.content
    
    # L·∫•y th√¥ng tin token usage
    usage = completion.usage
    return json.loads(content), usage


async def extract_with_groq(user_text: str):
    try:
        data, usage = await run_in_threadpool(_call_groq, user_text)
        return data, usage
    except Exception as e:
        error_msg = str(e)
        if "rate_limit_exceeded" in error_msg:
            print(f"\nüö´ RATE LIMIT HIT!")
            if "tokens per day" in error_msg.lower():
                print(f"   TPD limit reached. Need to wait or upgrade.")
            elif "requests per minute" in error_msg.lower():
                print(f"   RPM limit reached. Slowing down...")
        print(f"‚ùå Error: {error_msg[:200]}")
        return None, None


# ====== CSV translate ======
INPUT_CSV = "INPUT.csv"
OUTPUT_CSV = "OUTPUT.csv"
CHECKPOINT_CSV = "CHECKPOINT.csv"


async def translate_csv(max_rows=None):
    """
    D·ªãch CSV v·ªõi token tracking v√† rate limiting
    
    Args:
        max_rows: S·ªë d√≤ng t·ªëi ƒëa mu·ªën d·ªãch (None = kh√¥ng gi·ªõi h·∫°n)
    """
    
    # ƒê·ªçc t·∫•t c·∫£ rows
    print(f"üìñ Reading {INPUT_CSV}...")
    all_rows = []
    with open(INPUT_CSV, newline='', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        all_rows = list(reader)
    
    total_rows = len(all_rows)
    print(f"‚úÖ Found {total_rows} rows")
    
    # √Åp d·ª•ng gi·ªõi h·∫°n n·∫øu c√≥
    if max_rows:
        total_rows = min(total_rows, max_rows)
        all_rows = all_rows[:total_rows]
        print(f"‚ö†Ô∏è  Limiting to {total_rows} rows (token budget)")
    
    # Ki·ªÉm tra checkpoint
    translated_rows = []
    start_index = 0
    # total_tokens_used = 0
    print("üîÅ Always translating from beginning (no checkpoint used)")
    
    # try:
    #     with open(CHECKPOINT_CSV, newline='', encoding='utf-8') as f:
    #         reader = csv.DictReader(f)
    #         translated_rows = list(reader)
    #         start_index = len(translated_rows)
    #         print(f"üìå Resuming from row {start_index}")
    # except FileNotFoundError:
    #     print("üÜï Starting fresh translation")
    
    # Token tracking
    print(f"\nüí° Token budget: {MAX_TOKENS_PER_DAY:,} tokens/day")
    print(f"üí° Estimated: ~{ESTIMATED_TOKENS_PER_ROW} tokens/row")
    print(f"üí° Safe limit: {MAX_ROWS_PER_DAY} rows/day")

    total_tokens_used = 0

    
    # D·ªãch t·ª´ng batch
    for batch_num in range(start_index // BATCH_SIZE, (total_rows + BATCH_SIZE - 1) // BATCH_SIZE):
        batch_start = batch_num * BATCH_SIZE
        batch_end = min(batch_start + BATCH_SIZE, total_rows)
        batch_rows = all_rows[batch_start:batch_end]
        
        print(f"\n{'='*60}")
        print(f"üîÑ Batch {batch_num + 1} (rows {batch_start + 1}-{batch_end})")
        print(f"   Tokens used so far: {total_tokens_used:,}")
        print(f"{'='*60}")
        
        batch_start_time = time.time()
        batch_tokens = 0
        
        # D·ªãch t·ª´ng row
        for i, row in enumerate(batch_rows):
            current_row = batch_start + i + 1
            
            title = row.get("title", "")
            desc = row.get("description", "")
            
            # ‚úÖ KI·ªÇM TRA description d√†i
            desc_length = len(desc)
            if desc_length > 3000:
                print(f"  ‚ö†Ô∏è  Row {current_row}: Long description ({desc_length} chars), splitting...")
                
                # Chia description theo separator "|||"
                desc_parts = desc.split("|||")
                translated_parts = []
                
                for part_idx, part in enumerate(desc_parts):
                    if not part.strip():
                        translated_parts.append("")
                        continue
                        
                    json_text = json.dumps({
                        "title": "" if part_idx > 0 else title,  # Ch·ªâ d·ªãch title ·ªü part ƒë·∫ßu
                        "description": part.strip()
                    }, ensure_ascii=False)
                    
                    translated, usage = await extract_with_groq(json_text)
                    
                    if translated and usage:
                        batch_tokens += usage.total_tokens
                        total_tokens_used += usage.total_tokens
                        translated_parts.append(translated.get("description", part))
                        
                        if part_idx == 0 and translated.get("title"):
                            title = translated.get("title")
                        
                        print(f"     Part {part_idx + 1}/{len(desc_parts)}: {usage.total_tokens} tokens")
                    else:
                        translated_parts.append(part)
                    
                    # Delay nh·ªè gi·ªØa c√°c parts
                    await asyncio.sleep(0.5)
                
                # Merge l·∫°i
                new_row = row.copy()
                new_row["title"] = title
                new_row["description"] = "|||".join(translated_parts)
                translated_rows.append(new_row)
                
                print(f"  ‚úÖ Row {current_row}/{total_rows}: {title[:40]}... (split into {len(desc_parts)} parts)")
                
            else:
                # Description ng·∫Øn - d·ªãch b√¨nh th∆∞·ªùng
                json_text = json.dumps({
                    "title": title,
                    "description": desc
                }, ensure_ascii=False)
                
                translated, usage = await extract_with_groq(json_text)
                
                if translated and usage:
                    batch_tokens += usage.total_tokens
                    total_tokens_used += usage.total_tokens
                    
                    new_row = row.copy()
                    new_row["title"] = translated.get("title", title)
                    new_row["description"] = translated.get("description", desc)
                    translated_rows.append(new_row)
                    
                    title_preview = new_row['title'][:40] + "..." if len(new_row['title']) > 40 else new_row['title']
                    print(f"  ‚úÖ Row {current_row}/{total_rows}: {title_preview}")
                    print(f"     Tokens: {usage.total_tokens} (total: {total_tokens_used:,})")
                else:
                    translated_rows.append(row)
                    print(f"  ‚ö†Ô∏è  Row {current_row}/{total_rows}: Failed, keeping original")
            
            # KI·ªÇM TRA token limit
            if total_tokens_used >= MAX_TOKENS_PER_DAY:
                print(f"\n‚ö†Ô∏è  TOKEN LIMIT REACHED ({total_tokens_used:,}/{MAX_TOKENS_PER_DAY:,})")
                print(f"   Saving progress and stopping...")
                # save_checkpoint(translated_rows)
                save_final_output(translated_rows)
                print(f"\nüìä Translated {len(translated_rows)}/{len(all_rows)} rows")
                print(f"üíæ Resume tomorrow or upgrade plan!")
                return
            
            # L∆∞u checkpoint m·ªói 5 rows
            # if current_row % 5 == 0:
            #     save_checkpoint(translated_rows)
        
        # Batch summary
        batch_elapsed = time.time() - batch_start_time
        print(f"\nüìä Batch summary:")
        print(f"   Tokens used: {batch_tokens}")
        print(f"   Time: {batch_elapsed:.1f}s")
        
        # Delay gi·ªØa c√°c batch
        if batch_end < total_rows:
            wait_time = max(0, DELAY_BETWEEN_BATCHES - batch_elapsed)
            if wait_time > 0:
                print(f"‚è≥ Waiting {wait_time:.1f}s...")
                await asyncio.sleep(wait_time)
    
    # L∆∞u k·∫øt qu·∫£ cu·ªëi
    save_final_output(translated_rows)
    
    # ‚úÖ VALIDATION: Ki·ªÉm tra s·ªë c·ªôt
    print(f"\nüîç Validating output CSV...")
    try:
        with open(OUTPUT_CSV, newline='', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            output_rows = list(reader)
            
        print(f"‚úÖ Validation passed:")
        print(f"   Input rows: {len(all_rows)}")
        print(f"   Output rows: {len(output_rows)}")
        print(f"   Columns: {list(output_rows[0].keys()) if output_rows else 'N/A'}")
        
        # Ki·ªÉm tra c·ªôt
        if output_rows:
            expected_cols = list(all_rows[0].keys())
            actual_cols = list(output_rows[0].keys())
            if expected_cols != actual_cols:
                print(f"‚ö†Ô∏è  WARNING: Column mismatch!")
                print(f"   Expected: {expected_cols}")
                print(f"   Got: {actual_cols}")
    except Exception as e:
        print(f"‚ùå Validation failed: {e}")
    
    print(f"\nüéâ Translation complete!")
    print(f"üìä Final stats:")
    print(f"   Rows: {len(translated_rows)}/{len(all_rows)}")
    print(f"   Tokens: {total_tokens_used:,}")


def save_checkpoint(rows):
    """L∆∞u checkpoint - ƒê·∫¢M B·∫¢O format CSV ƒë√∫ng"""
    if rows:
        fieldnames = rows[0].keys()
        with open(CHECKPOINT_CSV, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)
            writer.writeheader()
            for row in rows:
                # ‚úÖ L√†m s·∫°ch description: x√≥a xu·ªëng d√≤ng kh√¥ng mong mu·ªën
                if 'description' in row:
                    desc = row['description']
                    # Gi·ªØ nguy√™n separator "|||", nh∆∞ng x√≥a \n trong m·ªói ph·∫ßn
                    parts = desc.split('|||')
                    cleaned_parts = []
                    for part in parts:
                        # X√≥a xu·ªëng d√≤ng, gi·ªØ m·ªôt kho·∫£ng tr·∫Øng
                        cleaned = ' '.join(part.split())
                        cleaned_parts.append(cleaned)
                    row['description'] = '|||'.join(cleaned_parts)
                
                writer.writerow(row)


def save_final_output(rows):
    """L∆∞u file cu·ªëi c√πng - ƒê·∫¢M B·∫¢O format CSV ƒë√∫ng"""
    if rows:
        fieldnames = rows[0].keys()
        with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)
            writer.writeheader()
            for row in rows:
                # ‚úÖ L√†m s·∫°ch description
                if 'description' in row:
                    desc = row['description']
                    parts = desc.split('|||')
                    cleaned_parts = []
                    for part in parts:
                        cleaned = ' '.join(part.split())
                        cleaned_parts.append(cleaned)
                    row['description'] = '|||'.join(cleaned_parts)
                
                writer.writerow(row)
        print(f"üíæ Saved to {OUTPUT_CSV}")


# ====== Run ======
if __name__ == "__main__":
    print("=" * 60)
    print("üåè VIETNAM PLACES CSV TRANSLATOR")
    print("=" * 60)
    print(f"‚öôÔ∏è  Model: {MODEL}")
    print(f"‚öôÔ∏è  Token limit: {MAX_TOKENS_PER_DAY:,}/day")
    print(f"‚öôÔ∏è  Safe rows/day: ~{MAX_ROWS_PER_DAY}")
    print("=" * 60)
    
    # T√ôY CH·ªåN: Gi·ªõi h·∫°n s·ªë d√≤ng d·ªãch m·ªói l·∫ßn ch·∫°y
    # Uncomment d√≤ng d∆∞·ªõi n·∫øu mu·ªën d·ªãch t·ªëi ƒëa N d√≤ng/ng√†y
    # MAX_ROWS_TO_TRANSLATE = 600  
    MAX_ROWS_TO_TRANSLATE = None  # None = d·ªãch h·∫øt (ƒë·∫øn khi h·∫øt token)
    
    start_time = datetime.now()
    asyncio.run(translate_csv(max_rows=MAX_ROWS_TO_TRANSLATE))
    end_time = datetime.now()
    
    elapsed = (end_time - start_time).total_seconds()
    print(f"\n‚è±Ô∏è  Total time: {elapsed/60:.1f} minutes")