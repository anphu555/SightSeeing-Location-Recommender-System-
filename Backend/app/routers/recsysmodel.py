import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # T·∫Øt warning CUDA
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Force TensorFlow d√πng CPU

import tensorflow as tf
import pickle
import pandas as pd
import numpy as np
import ast
import warnings

# T·∫Øt c·∫£nh b√°o sklearn version
warnings.filterwarnings('ignore', category=UserWarning)

# --- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ---
# ƒê·∫£m b·∫£o b·∫°n ƒë√£ c√≥ ƒë·ªß 3 file pickle/keras n√†y
MODEL_PATH = 'app/routers/two_tower_model.keras'
MLB_PATH = 'app/routers/mlb_vocab.pkl'
PROVINCE_MAP_PATH = 'app/routers/province_map.pkl' # <-- FILE M·ªöI
PLACES_CSV_PATH = 'app/services/vietnam_tourism_data_200tags_with_province.csv' # <-- D√πng file c√≥ province

# Bi·∫øn to√†n c·ª•c
loaded_model = None
loaded_mlb = None
loaded_province_map = None # <-- Bi·∫øn m·ªõi
item_features_cache = None # L∆∞u cache (tags_vec, province_id) c·ªßa t·∫•t c·∫£ items
places_df = None

def load_resources():
    """H√†m n√†y n√™n ƒë∆∞·ª£c g·ªçi 1 l·∫ßn khi start server"""
    global loaded_model, loaded_mlb, loaded_province_map, item_features_cache, places_df
    
    if loaded_model is None:
        print("Loading Two-Tower Model System...")
        
        # 1. Load Model & Disctionaries
        try:
            loaded_model = tf.keras.models.load_model(MODEL_PATH)
            with open(MLB_PATH, 'rb') as f:
                loaded_mlb = pickle.load(f)
            with open(PROVINCE_MAP_PATH, 'rb') as f:
                loaded_province_map = pickle.load(f)
        except Exception as e:
            print(f"L·ªói khi load model/pickle: {e}")
            return

        # 2. Load Data Places
        places_df = pd.read_csv(PLACES_CSV_PATH)
        
        # 3. Pre-process Data (T√°ch T·ªânh & Tags gi·ªëng h·ªát l√∫c Train)
        # H√†m parse tags
        def parse_and_split(x):
            try:
                tags = ast.literal_eval(x)
                if not tags: return "Unknown", []
                # Quy ∆∞·ªõc: Tag ƒë·∫ßu ti√™n l√† T·ªânh
                return tags[0], tags[1:] 
            except:
                return "Unknown", []

        # T√°ch c·ªôt
        print("Processing Places Data...")
        places_df[['province', 'clean_tags']] = places_df['tags'].apply(
            lambda x: pd.Series(parse_and_split(x))
        )
        
        # 4. T·∫°o Cache Features cho to√†n b·ªô Items (ƒë·ªÉ predict cho nhanh)
        # a. Tags Vector (One-hot)
        all_item_tags_vec = loaded_mlb.transform(places_df['clean_tags'])
        
        # b. Province Index
        # Map t√™n t·ªânh sang ID. N·∫øu t·ªânh m·ªõi l·∫° ko c√≥ trong map th√¨ g√°n 0
        all_item_prov_idx = places_df['province'].apply(
            lambda x: loaded_province_map.get(x, 0)
        ).values
        
        # L∆∞u v√†o cache ƒë·ªÉ d√πng cho h√†m predict
        item_features_cache = {
            'item_tags_input': all_item_tags_vec,
            'item_province_input': all_item_prov_idx
        }
        
        print("‚úÖ Load Resources Complete!")

def get_recommendations(user_prefs_tags, top_k=10):
    """
    H√†m g·ª£i √Ω cho 1 user d·ª±a tr√™n tags s·ªü th√≠ch.
    Args:
        user_prefs_tags (list): List c√°c tags user th√≠ch (VD: ['Nature', 'Cave'])
    """
    global loaded_model, loaded_mlb, item_features_cache, places_df
    
    if loaded_model is None:
        load_resources()
        
    # 1. Chu·∫©n b·ªã Input cho User Tower
    # Bi·∫øn ƒë·ªïi tags user nh·∫≠p v√†o th√†nh vector
    user_vec = loaded_mlb.transform([user_prefs_tags]) # Shape (1, num_features)
    
    # L·∫∑p l·∫°i vector user cho b·∫±ng s·ªë l∆∞·ª£ng items ƒë·ªÉ ƒë∆∞a v√†o model
    num_items = len(places_df)
    user_vec_repeated = np.repeat(user_vec, num_items, axis=0)
    
    # 2. D·ª± ƒëo√°n (Batch Predict)
    # Input dictionary ph·∫£i ƒë√∫ng t√™n layer trong model
    inputs = {
        'user_input': user_vec_repeated,
        'item_tags_input': item_features_cache['item_tags_input'],
        'item_province_input': item_features_cache['item_province_input']
    }
    
    predictions = loaded_model.predict(inputs, batch_size=512, verbose=0)
    
    # 3. L·∫•y k·∫øt qu·∫£
    scores = predictions.flatten()
    
    # T·∫°o b·∫£ng k·∫øt qu·∫£ t·∫°m
    results = places_df.copy()
    results['score'] = scores
    
    # Sort v√† l·∫•y top K
    top_results = results.sort_values(by='score', ascending=False).head(top_k)
    
    # Tr·∫£ v·ªÅ c√°c tr∆∞·ªùng c·∫ßn thi·∫øt (json)
    return top_results[['id', 'name', 'province', 'score']].to_dict(orient='records')

def recommend_two_tower(user_prefs_tags, top_k=10):
    """
    Wrapper function for get_recommendations to match the import name.
    Returns a DataFrame instead of dict for compatibility with recommendation.py
    
    Args:
        user_prefs_tags (list): List c√°c tags user th√≠ch
        top_k (int): S·ªë l∆∞·ª£ng g·ª£i √Ω tr·∫£ v·ªÅ
    
    Returns:
        pd.DataFrame: DataFrame ch·ª©a c√°c ƒë·ªãa ƒëi·ªÉm ƒë∆∞·ª£c g·ª£i √Ω
    """
    global loaded_model, loaded_mlb, item_features_cache, places_df
    
    if loaded_model is None:
        load_resources()
    
    # MAPPING: T·ª´ user input th√¥ng th∆∞·ªùng sang tags trong vocabulary
    # V√¨ MLB vocabulary ch·ª©a c√°c tags c·ª• th·ªÉ t·ª´ dataset
    tag_mapping = {
        'beach': ['Beach', 'Coastal', 'Sea', 'Island', 'Ocean'],
        'mountain': ['Mountain', 'Highland', 'Hill', 'Trekking', 'Hiking'],
        'nature': ['Nature', 'Natural', 'Ecotourism', 'Wildlife'],
        'historical': ['Historical', 'History', 'Heritage', 'Cultural Heritage'],
        'temple': ['Temple', 'Pagoda', 'Religious', 'Spiritual'],
        'city': ['City', 'Urban', 'Modern'],
        'food': ['Cuisine', 'Food', 'Restaurant'],
        'adventure': ['Adventure', 'Outdoor', 'Sports'],
        'relaxing': ['Relaxing', 'Peaceful', 'Quiet'],
        'festival': ['Festival', 'Event', 'Celebration']
    }
    
    # Expand user tags v·ªõi mapping
    expanded_tags = []
    vocab_set = set(loaded_mlb.classes_)
    
    for tag in user_prefs_tags:
        tag_lower = tag.lower()
        
        # 1. Th·ª≠ tag g·ªëc (capitalize)
        if tag in vocab_set:
            expanded_tags.append(tag)
        elif tag.capitalize() in vocab_set:
            expanded_tags.append(tag.capitalize())
        
        # 2. Th·ª≠ mapping
        if tag_lower in tag_mapping:
            for mapped_tag in tag_mapping[tag_lower]:
                if mapped_tag in vocab_set:
                    expanded_tags.append(mapped_tag)
    
    # N·∫øu kh√¥ng match ƒë∆∞·ª£c g√¨, d√πng tags ph·ªï bi·∫øn
    if not expanded_tags:
        print(f"‚ö†Ô∏è Warning: Tags {user_prefs_tags} not found in vocabulary. Using default.")
        expanded_tags = ['Vietnam', 'Sightseeing', 'Cultural']
    
    print(f"üîç User input: {user_prefs_tags} ‚Üí Expanded: {expanded_tags}")
        
    # 1. Chu·∫©n b·ªã Input cho User Tower
    user_vec = loaded_mlb.transform([expanded_tags])
    
    # 2. L·∫∑p l·∫°i vector user cho b·∫±ng s·ªë l∆∞·ª£ng items
    num_items = len(places_df)
    user_vec_repeated = np.repeat(user_vec, num_items, axis=0)
    
    # 3. D·ª± ƒëo√°n
    inputs = {
        'user_input': user_vec_repeated,
        'item_tags_input': item_features_cache['item_tags_input'],
        'item_province_input': item_features_cache['item_province_input']
    }
    
    predictions = loaded_model.predict(inputs, batch_size=512, verbose=0)
    
    # 4. L·∫•y k·∫øt qu·∫£
    scores = predictions.flatten()
    
    # T·∫°o DataFrame k·∫øt qu·∫£
    results = places_df.copy()
    results['score'] = scores
    
    # Sort v√† l·∫•y top K
    top_results = results.sort_values(by='score', ascending=False).head(top_k)
    
    # Parse tags t·ª´ string sang list tr∆∞·ªõc khi tr·∫£ v·ªÅ
    def safe_parse_tags(x):
        if isinstance(x, str):
            try:
                return ast.literal_eval(x)
            except:
                return []
        elif isinstance(x, list):
            return x
        else:
            return []
    
    # T·∫°o b·∫£n copy v√† parse tags
    final_results = top_results.copy()
    final_results['tags'] = final_results['tags'].apply(safe_parse_tags)
    
    # Tr·∫£ v·ªÅ DataFrame (kh√¥ng ph·∫£i dict)
    return final_results[['id', 'name', 'tags', 'province', 'score']]

# --- TEST CODE (Ch·∫°y th·ª≠ khi execute file n√†y) ---
if __name__ == "__main__":
    load_resources()
    
    # Gi·∫£ l·∫≠p user th√≠ch ƒëi Ch√πa & L·ªÖ h·ªôi
    demo_tags = ['Temple', 'Festival', 'Historical']
    print(f"\nUser th√≠ch: {demo_tags}")
    
    recs = get_recommendations(demo_tags)
    for r in recs:
        print(f"- [{r['province']}] {r['name']} (Score: {r['score']:.2f})")









# import pandas as pd
# import numpy as np
# from sklearn.feature_extraction.text import CountVectorizer
# from sklearn.metrics.pairwise import cosine_similarity
# from sqlmodel import Session, select
# from typing import Optional

# from app.schemas import Place, Rating
# from app.database import engine

# # ==========================================
# # 1. LOAD D·ªÆ LI·ªÜU T·ª™ DATABASE.DB
# # ==========================================

# def load_places_from_db():
#     """Load t·∫•t c·∫£ places t·ª´ database.db v√†o DataFrame"""
#     with Session(engine) as session:
#         statement = select(Place)
#         places = session.exec(statement).all()
        
#         # Chuy·ªÉn ƒë·ªïi sang list of dict
#         places_data = []
#         for place in places:
#             # K·∫øt h·ª£p c√°c fields th√†nh text ƒë·ªÉ vectorize
#             # tags l√† List[str], description c≈©ng l√† List[str]
#             tags_text = " ".join(place.tags) if place.tags else ""
#             desc_text = " ".join(place.description) if place.description else ""
            
#             places_data.append({
#                 "id": place.id,
#                 "name": place.name,
#                 "tags": place.tags,
#                 "description": place.description,
#                 "images": place.image,
#                 # T·∫°o soup ƒë·ªÉ vectorize
#                 "soup": f"{place.name} {tags_text} {desc_text}"
#             })
        
#         return pd.DataFrame(places_data)

# # LAZY LOADING: Ch·ªâ load khi c·∫ßn, kh√¥ng load ngay khi import module
# items_df = None
# count_matrix = None
# vectorizer = None

# def initialize_recsys():
#     """Kh·ªüi t·∫°o RecSys model - g·ªçi h√†m n√†y sau khi database ƒë√£ ƒë∆∞·ª£c t·∫°o"""
#     global items_df, count_matrix, vectorizer
    
#     if items_df is not None:
#         return  # ƒê√£ kh·ªüi t·∫°o r·ªìi
    
#     try:
#         # Load data t·ª´ database
#         items_df = load_places_from_db()
        
#         if len(items_df) == 0:
#             print("Warning: No places found in database")
#             return
        
#         # Kh·ªüi t·∫°o vectorizer
#         vectorizer = CountVectorizer(stop_words='english', max_features=5000)
#         count_matrix = vectorizer.fit_transform(items_df['soup'])
        
#         print(f"RecSys initialized with {len(items_df)} places")
#     except Exception as e:
#         print(f"Failed to initialize RecSys: {e}")
#         items_df = pd.DataFrame()  # Empty dataframe ƒë·ªÉ tr√°nh l·ªói

# # --- H√ÄM H·ªñ TR·ª¢ ---

# def get_item_vector(item_id):
#     """L·∫•y vector c·ªßa m·ªôt ƒë·ªãa ƒëi·ªÉm d·ª±a tr√™n ID"""
#     if count_matrix is None:
#         return None
#     try:
#         idx = items_df.index[items_df['id'] == item_id].tolist()[0]
#         return count_matrix[idx]
#     except (IndexError, KeyError):
#         return None

# def build_user_profile(user_id: int):
#     """
#     T·∫°o vector s·ªü th√≠ch ng∆∞·ªùi d√πng d·ª±a tr√™n Rating history
#     Score cao (4-5) ‚Üí Positive influence
#     Score th·∫•p (1-2) ‚Üí Negative influence
#     """
#     with Session(engine) as session:
#         statement = select(Rating).where(Rating.user_id == user_id)
#         ratings = session.exec(statement).all()
        
#         if not ratings:
#             return None  # User m·ªõi ho√†n to√†n (Cold start)

#     # Kh·ªüi t·∫°o vector r·ªóng
#     if count_matrix is None:
#         return None
        
#     user_profile = np.zeros(count_matrix.shape[1])
#     interaction_count = 0
    
#     for rating in ratings:
#         item_vec = get_item_vector(rating.place_id)
#         if item_vec is not None:
#             # Chuy·ªÉn ƒë·ªïi score (1-5) th√†nh weight (-1 ƒë·∫øn +1)
#             # Score 5 ‚Üí +1, Score 3 ‚Üí 0, Score 1 ‚Üí -1
#             weight = (rating.score - 3.0) / 2.0  # Normalize v·ªÅ [-1, 1]
            
#             user_profile += weight * item_vec.toarray()[0]
#             interaction_count += 1
            
#     if interaction_count == 0:
#         return None
        
#     return user_profile

# # 4. H√ÄM RECOMMEND CH√çNH
# def recommend(user_prompt_extraction, user_id: Optional[int] = None):
#     """
#     user_prompt_extraction: K·∫øt qu·∫£ JSON t·ª´ LLM (user_text)
#     user_id: ID ng∆∞·ªùi d√πng ƒë·ªÉ l·∫•y l·ªãch s·ª≠ ratings
#     """
#     # ƒê·∫£m b·∫£o RecSys ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o
#     initialize_recsys()
    
#     if count_matrix is None or items_df is None or len(items_df) == 0:
#         return pd.DataFrame()  # Return empty dataframe
    
#     # --- B∆Ø·ªöC 1: X√ÇY D·ª∞NG QUERY VECTOR T·ª™ PROMPT ---
#     search_keywords = []
    
#     # L·∫•y th√¥ng tin t·ª´ extraction
#     if user_prompt_extraction.type and user_prompt_extraction.type != 'unknown':
#         search_keywords.append(user_prompt_extraction.type)
        
#     for loc in user_prompt_extraction.location:
#         search_keywords.append(loc)
        
#     if user_prompt_extraction.weather and user_prompt_extraction.weather != 'unknown':
#         search_keywords.append(user_prompt_extraction.weather)

#     search_query = " ".join(search_keywords)
    
#     # T·∫°o vector t·ª´ prompt
#     try:
#         query_vec = vectorizer.transform([search_query]).toarray()[0]
#     except:
#         query_vec = np.zeros(count_matrix.shape[1])

#     # --- B∆Ø·ªöC 2: K·∫æT H·ª¢P V·ªöI L·ªäCH S·ª¨ USER (N·∫æU C√ì) ---
#     if user_id:
#         user_profile_vec = build_user_profile(user_id)
#         if user_profile_vec is not None:
#             # HYBRID: 70% Prompt + 30% User History
#             final_vec = (query_vec * 0.7) + (user_profile_vec * 0.3)
#         else:
#             final_vec = query_vec
#     else:
#         final_vec = query_vec

#     # --- B∆Ø·ªöC 3: T√çNH TO√ÅN ---
#     if np.all(final_vec == 0):
#         # Kh√¥ng c√≥ prompt, kh√¥ng c√≥ history ‚Üí Tr·∫£ v·ªÅ top items
#         results = items_df.copy()
#         results['score'] = 0.5  # Default score
#         return results.head(10)

#     # T√≠nh Cosine Similarity
#     cosine_sim = cosine_similarity([final_vec], count_matrix)
#     scores = cosine_sim[0]
    
#     # T·∫°o b·∫£ng k·∫øt qu·∫£
#     results = items_df.copy()
#     results['score'] = scores
    
#     # --- B∆Ø·ªöC 4: L·ªåC THEO LOCATION (N·∫æU C√ì) ---
#     # Note: Schema m·ªõi kh√¥ng c√≥ tr∆∞·ªùng 'province', ch·ªâ c√≥ 'tags'
#     # B·∫°n c√≥ th·ªÉ l·ªçc theo tags n·∫øu tags ch·ª©a t√™n ƒë·ªãa danh
#     if user_prompt_extraction.location:
#         user_locations_lower = [loc.lower().strip() for loc in user_prompt_extraction.location]
        
#         def matches_location(tags_list):
#             if not tags_list:
#                 return False
#             tags_lower = [tag.lower() for tag in tags_list]
#             return any(
#                 any(user_loc in tag for user_loc in user_locations_lower)
#                 for tag in tags_lower
#             )
        
#         # Filter places c√≥ tags ch·ª©a location
#         mask = results['tags'].apply(matches_location)
#         filtered = results[mask]
        
#         # N·∫øu filter qu√° ch·∫∑t (kh√¥ng c√≤n k·∫øt qu·∫£), gi·ªØ nguy√™n
#         if len(filtered) > 0:
#             results = filtered

#     # S·∫Øp x·∫øp gi·∫£m d·∫ßn theo score
#     results = results.sort_values(by='score', ascending=False)
    
#     return results