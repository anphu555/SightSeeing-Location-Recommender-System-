"""
EVALUATION FRAMEWORK CHO H·ªÜ TH·ªêNG ƒê·ªÄ XU·∫§T (RECOMMENDATION SYSTEM)
=====================================================================

Framework n√†y ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa thu·∫≠t to√°n ƒë·ªÅ xu·∫•t d·ª±a tr√™n:
1. Precision@K, Recall@K, F1@K
2. Mean Average Precision (MAP)
3. Normalized Discounted Cumulative Gain (NDCG)
4. Coverage & Diversity
5. Cold-start performance

Author: Evaluation System
Date: December 16, 2025
"""

import pandas as pd
import numpy as np
from sqlmodel import Session, select
from typing import List, Dict, Tuple, Optional
from collections import defaultdict
import json

import sys
import os
# Add backend directory to path
sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

from app.database import engine
from app.schemas import User, Place, Rating, Like
from app.routers.recsysmodel import recommend_two_tower, initialize_recsys

# ==========================================
# 1. T·∫†O TEST SET
# ==========================================

class TestSetGenerator:
    """T·∫°o test set t·ª´ d·ªØ li·ªáu hi·ªán c√≥"""
    
    def __init__(self, session: Session):
        self.session = session
        self.train_ratings = []
        self.test_ratings = []
        
    def create_train_test_split(self, test_ratio=0.2, min_interactions=5):
        """
        T·∫°o train/test split theo Leave-One-Out ho·∫∑c Random Split
        
        Args:
            test_ratio: T·ªâ l·ªá d·ªØ li·ªáu ƒë·ªÉ test (0.2 = 20%)
            min_interactions: User ph·∫£i c√≥ √≠t nh·∫•t n interactions ƒë·ªÉ ƒë∆∞·ª£c ƒë∆∞a v√†o test
            
        Returns:
            train_data, test_data
        """
        # L·∫•y t·∫•t c·∫£ ratings v√† likes
        all_ratings = self.session.exec(select(Rating)).all()
        all_likes = self.session.exec(select(Like).where(Like.place_id.isnot(None))).all()
        
        # T·ªïng h·ª£p interactions theo user
        user_interactions = defaultdict(list)
        
        # T·ª´ ratings (score >= 3.0 coi l√† positive)
        for rating in all_ratings:
            if rating.score >= 3.0:
                user_interactions[rating.user_id].append({
                    'user_id': rating.user_id,
                    'place_id': rating.place_id,
                    'score': rating.score,
                    'type': 'rating'
                })
        
        # T·ª´ likes (is_like=True coi l√† positive)
        for like in all_likes:
            if like.is_like:
                user_interactions[like.user_id].append({
                    'user_id': like.user_id,
                    'place_id': like.place_id,
                    'score': 5.0,  # Like = score cao nh·∫•t
                    'type': 'like'
                })
        
        # L·ªçc users c√≥ ƒë·ªß interactions
        qualified_users = {
            user_id: interactions 
            for user_id, interactions in user_interactions.items() 
            if len(interactions) >= min_interactions
        }
        
        print(f"‚úì T·ªïng s·ªë users: {len(user_interactions)}")
        print(f"‚úì Users c√≥ ƒë·ªß {min_interactions}+ interactions: {len(qualified_users)}")
        
        # Split data
        train_data = []
        test_data = []
        
        for user_id, interactions in qualified_users.items():
            # Shuffle ƒë·ªÉ random
            interactions = list(interactions)
            np.random.shuffle(interactions)
            
            # T√≠nh s·ªë l∆∞·ª£ng test items
            n_test = max(1, int(len(interactions) * test_ratio))
            
            test_data.extend(interactions[:n_test])
            train_data.extend(interactions[n_test:])
        
        self.train_ratings = train_data
        self.test_ratings = test_data
        
        print(f"‚úì Train set: {len(train_data)} interactions")
        print(f"‚úì Test set: {len(test_data)} interactions")
        print(f"‚úì Test users: {len(set([t['user_id'] for t in test_data]))}")
        
        return train_data, test_data
    
    def get_ground_truth(self) -> Dict[int, List[int]]:
        """
        L·∫•y ground truth: Danh s√°ch places m√† m·ªói user th·ª±c s·ª± th√≠ch
        
        Returns:
            Dict[user_id] = [place_id1, place_id2, ...]
        """
        ground_truth = defaultdict(list)
        
        for item in self.test_ratings:
            ground_truth[item['user_id']].append(item['place_id'])
        
        return dict(ground_truth)

# ==========================================
# 2. METRICS ƒê√ÅNH GI√Å
# ==========================================

class RecommendationMetrics:
    """C√°c metrics ƒë·ªÉ ƒë√°nh gi√° recommendation system"""
    
    @staticmethod
    def precision_at_k(recommended: List[int], relevant: List[int], k: int) -> float:
        """
        Precision@K: T·ªâ l·ªá items ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t l√† relevant
        
        Formula: (# relevant items in top-k) / k
        """
        if k == 0:
            return 0.0
        
        recommended_k = recommended[:k]
        relevant_set = set(relevant)
        
        hits = len([item for item in recommended_k if item in relevant_set])
        return hits / k
    
    @staticmethod
    def recall_at_k(recommended: List[int], relevant: List[int], k: int) -> float:
        """
        Recall@K: T·ªâ l·ªá relevant items ƒë∆∞·ª£c t√¨m th·∫•y trong top-k
        
        Formula: (# relevant items in top-k) / (total # relevant items)
        """
        if len(relevant) == 0:
            return 0.0
        
        recommended_k = recommended[:k]
        relevant_set = set(relevant)
        
        hits = len([item for item in recommended_k if item in relevant_set])
        return hits / len(relevant)
    
    @staticmethod
    def f1_at_k(recommended: List[int], relevant: List[int], k: int) -> float:
        """
        F1@K: Harmonic mean c·ªßa Precision@K v√† Recall@K
        """
        precision = RecommendationMetrics.precision_at_k(recommended, relevant, k)
        recall = RecommendationMetrics.recall_at_k(recommended, relevant, k)
        
        if precision + recall == 0:
            return 0.0
        
        return 2 * (precision * recall) / (precision + recall)
    
    @staticmethod
    def average_precision(recommended: List[int], relevant: List[int]) -> float:
        """
        Average Precision (AP): Trung b√¨nh precision t·∫°i m·ªói relevant item
        
        Formula: (1/|relevant|) * Œ£(Precision@k * rel(k))
        """
        if len(relevant) == 0:
            return 0.0
        
        relevant_set = set(relevant)
        score = 0.0
        num_hits = 0.0
        
        for i, item in enumerate(recommended):
            if item in relevant_set:
                num_hits += 1.0
                precision_at_i = num_hits / (i + 1.0)
                score += precision_at_i
        
        return score / len(relevant)
    
    @staticmethod
    def ndcg_at_k(recommended: List[int], relevant: List[int], k: int) -> float:
        """
        Normalized Discounted Cumulative Gain (NDCG@K)
        ƒê√°nh gi√° ranking quality: items relevant ·ªü v·ªã tr√≠ cao h∆°n = t·ªët h∆°n
        
        Formula: DCG@K / IDCG@K
        """
        def dcg(scores: List[float], k: int) -> float:
            """Discounted Cumulative Gain"""
            scores_k = scores[:k]
            return sum([
                (2**score - 1) / np.log2(i + 2)  # i+2 v√¨ index b·∫Øt ƒë·∫ßu t·ª´ 0
                for i, score in enumerate(scores_k)
            ])
        
        # T·∫°o relevance scores (1 n·∫øu relevant, 0 n·∫øu kh√¥ng)
        relevant_set = set(relevant)
        scores = [1.0 if item in relevant_set else 0.0 for item in recommended]
        
        # DCG c·ªßa recommended list
        dcg_score = dcg(scores, k)
        
        # IDCG (ideal DCG): s·∫Øp x·∫øp t·∫•t c·∫£ relevant items l√™n ƒë·∫ßu
        ideal_scores = [1.0] * min(len(relevant), k) + [0.0] * max(0, k - len(relevant))
        idcg_score = dcg(ideal_scores, k)
        
        if idcg_score == 0:
            return 0.0
        
        return dcg_score / idcg_score
    
    @staticmethod
    def coverage(all_recommended: List[List[int]], total_items: int) -> float:
        """
        Coverage: T·ªâ l·ªá items ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t √≠t nh·∫•t 1 l·∫ßn
        
        Formula: (# unique items recommended) / (total # items)
        """
        unique_items = set()
        for rec_list in all_recommended:
            unique_items.update(rec_list)
        
        return len(unique_items) / total_items if total_items > 0 else 0.0
    
    @staticmethod
    def diversity(all_recommended: List[List[int]]) -> float:
        """
        Diversity: ƒêo ƒë·ªô ƒëa d·∫°ng c·ªßa c√°c recommendations
        T√≠nh b·∫±ng average pairwise distance gi·ªØa c√°c items
        
        ·ªû ƒë√¢y d√πng ph∆∞∆°ng ph√°p ƒë∆°n gi·∫£n: unique items / total recommended items
        """
        total_items = 0
        unique_items = set()
        
        for rec_list in all_recommended:
            total_items += len(rec_list)
            unique_items.update(rec_list)
        
        return len(unique_items) / total_items if total_items > 0 else 0.0

# ==========================================
# 3. EVALUATOR CH√çNH
# ==========================================

class RecommendationEvaluator:
    """Class ch√≠nh ƒë·ªÉ ƒë√°nh gi√° recommendation system"""
    
    def __init__(self, session: Session):
        self.session = session
        self.metrics = RecommendationMetrics()
        
    def evaluate_user(
        self, 
        user_id: int, 
        ground_truth: List[int],
        k_values: List[int] = [5, 10, 20]
    ) -> Dict:
        """
        ƒê√°nh gi√° cho 1 user c·ª• th·ªÉ
        
        Args:
            user_id: ID c·ªßa user
            ground_truth: List places m√† user th·ª±c s·ª± th√≠ch (t·ª´ test set)
            k_values: C√°c gi√° tr·ªã K ƒë·ªÉ ƒë√°nh gi√°
            
        Returns:
            Dict ch·ª©a c√°c metrics
        """
        # L·∫•y user preferences (t·ª´ history)
        user = self.session.get(User, user_id)
        if not user:
            return None
        
        # L·∫•y tags t·ª´ ratings history (score >= 3.0)
        statement = select(Rating).where(Rating.user_id == user_id, Rating.score >= 3.0)
        ratings = self.session.exec(statement).all()
        
        user_tags = []
        for rating in ratings:
            place = self.session.get(Place, rating.place_id)
            if place and place.tags:
                user_tags.extend(place.tags)
        
        # KH√îNG d√πng preferences trong evaluation (realistic test)
        # Ch·ªâ d√πng actual behavior (ratings/likes)
        
        # Remove duplicates
        user_tags = list(set(user_tags))
        
        # N·∫øu kh√¥ng c√≥ tags, d√πng popular recommendations
        if not user_tags:
            user_tags = []
        
        # G·ªçi recommendation model
        try:
            recommendations_df = recommend_two_tower(
                user_prefs_tags=user_tags,
                user_id=user_id,
                top_k=max(k_values)  # L·∫•y top-K l·ªõn nh·∫•t
            )
            
            recommended_ids = recommendations_df['id'].tolist()
        except Exception as e:
            print(f"‚úó Error recommending for user {user_id}: {e}")
            return None
        
        # T√≠nh metrics cho t·ª´ng K
        results = {'user_id': user_id}
        
        for k in k_values:
            results[f'precision@{k}'] = self.metrics.precision_at_k(recommended_ids, ground_truth, k)
            results[f'recall@{k}'] = self.metrics.recall_at_k(recommended_ids, ground_truth, k)
            results[f'f1@{k}'] = self.metrics.f1_at_k(recommended_ids, ground_truth, k)
            results[f'ndcg@{k}'] = self.metrics.ndcg_at_k(recommended_ids, ground_truth, k)
        
        results['map'] = self.metrics.average_precision(recommended_ids, ground_truth)
        results['num_relevant'] = len(ground_truth)
        results['num_recommended'] = len(recommended_ids)
        
        return results
    
    def evaluate_all(
        self,
        ground_truth_dict: Dict[int, List[int]],
        k_values: List[int] = [5, 10, 20]
    ) -> Dict:
        """
        ƒê√°nh gi√° to√†n b·ªô test set
        
        Args:
            ground_truth_dict: Dict[user_id] = [relevant_place_ids]
            k_values: C√°c gi√° tr·ªã K ƒë·ªÉ ƒë√°nh gi√°
            
        Returns:
            Dict ch·ª©a aggregate metrics
        """
        all_results = []
        all_recommended = []
        
        print(f"\n{'='*60}")
        print(f"ƒêANG ƒê√ÅNH GI√Å {len(ground_truth_dict)} USERS...")
        print(f"{'='*60}\n")
        
        for i, (user_id, relevant_places) in enumerate(ground_truth_dict.items(), 1):
            print(f"[{i}/{len(ground_truth_dict)}] User {user_id}: {len(relevant_places)} relevant places")
            
            result = self.evaluate_user(user_id, relevant_places, k_values)
            
            if result:
                all_results.append(result)
                
                # L∆∞u recommendations ƒë·ªÉ t√≠nh coverage/diversity
                user = self.session.get(User, user_id)
                if user:
                    user_tags = []
                    statement = select(Rating).where(Rating.user_id == user_id, Rating.score >= 3.0)
                    ratings = self.session.exec(statement).all()
                    for rating in ratings:
                        place = self.session.get(Place, rating.place_id)
                        if place and place.tags:
                            user_tags.extend(place.tags)
                    
                    # KH√îNG d√πng preferences - ch·ªâ actual behavior
                    
                    user_tags = list(set(user_tags))
                    
                    try:
                        recs_df = recommend_two_tower(user_tags, user_id, max(k_values))
                        all_recommended.append(recs_df['id'].tolist())
                    except:
                        pass
        
        if not all_results:
            print("‚úó Kh√¥ng c√≥ k·∫øt qu·∫£ ƒë√°nh gi√° n√†o!")
            return {}
        
        # T√≠nh aggregate metrics
        df_results = pd.DataFrame(all_results)
        
        aggregate = {
            'num_users_evaluated': len(all_results),
            'avg_relevant_per_user': df_results['num_relevant'].mean(),
        }
        
        # Average metrics
        for k in k_values:
            aggregate[f'avg_precision@{k}'] = df_results[f'precision@{k}'].mean()
            aggregate[f'avg_recall@{k}'] = df_results[f'recall@{k}'].mean()
            aggregate[f'avg_f1@{k}'] = df_results[f'f1@{k}'].mean()
            aggregate[f'avg_ndcg@{k}'] = df_results[f'ndcg@{k}'].mean()
        
        aggregate['avg_map'] = df_results['map'].mean()
        
        # Coverage & Diversity
        total_places = self.session.exec(select(Place)).all()
        aggregate['coverage'] = self.metrics.coverage(all_recommended, len(total_places))
        aggregate['diversity'] = self.metrics.diversity(all_recommended)
        
        return aggregate, df_results

# ==========================================
# 4. MAIN EVALUATION SCRIPT
# ==========================================

def run_evaluation(test_ratio=0.2, min_interactions=5, k_values=[5, 10, 20]):
    """
    Ch·∫°y full evaluation pipeline
    
    Args:
        test_ratio: T·ªâ l·ªá data cho test set
        min_interactions: S·ªë interactions t·ªëi thi·ªÉu c·ªßa user
        k_values: C√°c gi√° tr·ªã K ƒë·ªÉ ƒë√°nh gi√°
    """
    print(f"\n{'='*60}")
    print(f"H·ªÜ TH·ªêNG ƒê√ÅNH GI√Å RECOMMENDATION ALGORITHM")
    print(f"{'='*60}\n")
    
    # Kh·ªüi t·∫°o RecSys model
    print("‚è≥ ƒêang kh·ªüi t·∫°o Recommendation Model...")
    initialize_recsys()
    print("‚úì Model ƒë√£ s·∫µn s√†ng!\n")
    
    # T·∫°o session
    with Session(engine) as session:
        # B∆∞·ªõc 1: T·∫°o test set
        print("B∆Ø·ªöC 1: T·∫†O TRAIN/TEST SPLIT")
        print("-" * 60)
        
        generator = TestSetGenerator(session)
        train_data, test_data = generator.create_train_test_split(
            test_ratio=test_ratio,
            min_interactions=min_interactions
        )
        
        if not test_data:
            print("‚úó Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ t·∫°o test set!")
            print("üí° G·ª£i √Ω: C·∫ßn c√≥ √≠t nh·∫•t 1 user v·ªõi >= 5 interactions (ratings/likes)")
            return None
        
        ground_truth = generator.get_ground_truth()
        
        # B∆∞·ªõc 2: Ch·∫°y evaluation
        print(f"\nB∆Ø·ªöC 2: ƒê√ÅNH GI√Å V·ªöI K = {k_values}")
        print("-" * 60)
        
        evaluator = RecommendationEvaluator(session)
        aggregate_results, detailed_results = evaluator.evaluate_all(ground_truth, k_values)
        
        # B∆∞·ªõc 3: Hi·ªÉn th·ªã k·∫øt qu·∫£
        print(f"\n{'='*60}")
        print(f"K·∫æT QU·∫¢ ƒê√ÅNH GI√Å T·ªîNG H·ª¢P")
        print(f"{'='*60}\n")
        
        print(f"üìä S·ªë users ƒë∆∞·ª£c ƒë√°nh gi√°: {aggregate_results['num_users_evaluated']}")
        print(f"üìä Trung b√¨nh relevant items/user: {aggregate_results['avg_relevant_per_user']:.2f}\n")
        
        print("üìà PRECISION (ƒê·ªô ch√≠nh x√°c c·ªßa ƒë·ªÅ xu·∫•t):")
        for k in k_values:
            score = aggregate_results[f'avg_precision@{k}'] * 100
            print(f"   ‚Ä¢ Precision@{k}: {score:.2f}%")
        
        print("\nüìà RECALL (T·ªâ l·ªá items relevant ƒë∆∞·ª£c t√¨m th·∫•y):")
        for k in k_values:
            score = aggregate_results[f'avg_recall@{k}'] * 100
            print(f"   ‚Ä¢ Recall@{k}: {score:.2f}%")
        
        print("\nüìà F1 SCORE (Harmonic mean of Precision & Recall):")
        for k in k_values:
            score = aggregate_results[f'avg_f1@{k}'] * 100
            print(f"   ‚Ä¢ F1@{k}: {score:.2f}%")
        
        print("\nüìà NDCG (Ranking Quality):")
        for k in k_values:
            score = aggregate_results[f'avg_ndcg@{k}'] * 100
            print(f"   ‚Ä¢ NDCG@{k}: {score:.2f}%")
        
        map_score = aggregate_results['avg_map'] * 100
        print(f"\nüìà MAP (Mean Average Precision): {map_score:.2f}%")
        
        coverage = aggregate_results['coverage'] * 100
        diversity = aggregate_results['diversity'] * 100
        print(f"\nüìà COVERAGE (Catalog coverage): {coverage:.2f}%")
        print(f"üìà DIVERSITY (Recommendation diversity): {diversity:.2f}%")
        
        # ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng
        print(f"\n{'='*60}")
        print("üí° ƒê√ÅNH GI√Å CH·∫§T L∆Ø·ª¢NG THU·∫¨T TO√ÅN")
        print(f"{'='*60}\n")
        
        # Ti√™u ch√≠ ƒë√°nh gi√° (industry standards)
        avg_precision_10 = aggregate_results['avg_precision@10']
        avg_ndcg_10 = aggregate_results['avg_ndcg@10']
        
        if avg_precision_10 >= 0.3 and avg_ndcg_10 >= 0.4:
            quality = "üåü XU·∫§T S·∫ÆC"
        elif avg_precision_10 >= 0.2 and avg_ndcg_10 >= 0.3:
            quality = "‚úÖ T·ªêT"
        elif avg_precision_10 >= 0.1 and avg_ndcg_10 >= 0.2:
            quality = "‚ö†Ô∏è TRUNG B√åNH"
        else:
            quality = "‚ùå C·∫¶N C·∫¢I THI·ªÜN"
        
        print(f"K·∫øt lu·∫≠n: {quality}")
        
        if quality == "‚ùå C·∫¶N C·∫¢I THI·ªÜN":
            print("\nüí° G·ª£i √Ω c·∫£i thi·ªán:")
            print("   1. Thu th·∫≠p th√™m d·ªØ li·ªáu user interactions")
            print("   2. C·∫£i thi·ªán feature engineering (tags, descriptions)")
            print("   3. Th·ª≠ c√°c thu·∫≠t to√°n kh√°c (collaborative filtering, hybrid)")
            print("   4. Fine-tune hyperparameters")
        
        # L∆∞u k·∫øt qu·∫£
        print(f"\n{'='*60}")
        print("üíæ ƒêANG L∆ØU K·∫æT QU·∫¢...")
        print(f"{'='*60}\n")
        
        # L∆∞u aggregate results
        with open('evaluation_results.json', 'w', encoding='utf-8') as f:
            json.dump(aggregate_results, f, indent=2, ensure_ascii=False)
        print("‚úì ƒê√£ l∆∞u: evaluation_results.json")
        
        # L∆∞u detailed results
        detailed_results.to_csv('evaluation_detailed.csv', index=False, encoding='utf-8')
        print("‚úì ƒê√£ l∆∞u: evaluation_detailed.csv")
        
        return aggregate_results, detailed_results

# ==========================================
# 5. COLD-START EVALUATION
# ==========================================

def evaluate_cold_start():
    """
    ƒê√°nh gi√° hi·ªáu su·∫•t v·ªõi cold-start users (users m·ªõi, kh√¥ng c√≥ history)
    """
    print(f"\n{'='*60}")
    print("ƒê√ÅNH GI√Å COLD-START PERFORMANCE")
    print(f"{'='*60}\n")
    
    initialize_recsys()
    
    # Test scenarios cho cold-start users
    test_queries = [
        {"tags": ["Hanoi", "Historical"], "description": "User m·ªõi th√≠ch l·ªãch s·ª≠ H√† N·ªôi"},
        {"tags": ["Beach", "Nha Trang"], "description": "User m·ªõi mu·ªën ƒëi bi·ªÉn Nha Trang"},
        {"tags": ["Mountain", "Nature"], "description": "User m·ªõi th√≠ch n√∫i v√† thi√™n nhi√™n"},
        {"tags": ["Food", "Culture"], "description": "User m·ªõi quan t√¢m ·∫©m th·ª±c vƒÉn h√≥a"},
        {"tags": [], "description": "User m·ªõi kh√¥ng c√≥ preferences"}
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"Test {i}: {query['description']}")
        print(f"Tags: {query['tags']}")
        
        try:
            results = recommend_two_tower(query['tags'], user_id=None, top_k=5)
            print(f"‚úì Tr·∫£ v·ªÅ {len(results)} recommendations")
            print(f"  Places: {', '.join(results['name'].tolist()[:3])}...\n")
        except Exception as e:
            print(f"‚úó Error: {e}\n")

# ==========================================
# RUN EVALUATION
# ==========================================

if __name__ == "__main__":
    # Full evaluation
    results = run_evaluation(
        test_ratio=0.2,
        min_interactions=5,
        k_values=[5, 10, 20]
    )
    
    # Cold-start evaluation
    evaluate_cold_start()
